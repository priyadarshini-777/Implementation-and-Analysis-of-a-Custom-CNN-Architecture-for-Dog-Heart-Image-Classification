{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad82f056-fe64-4f81-b74e-49329905b97e",
      "metadata": {
        "id": "ad82f056-fe64-4f81-b74e-49329905b97e"
      },
      "source": [
        "# 1. Build your own convolutional neural network using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ta76xBMmqI_K"
      },
      "id": "ta76xBMmqI_K",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths to datasets\n",
        "train_dataset_path = '/content/drive/My Drive/Dog_Heart/Train'\n",
        "val_dataset_path = '/content/drive/My Drive/Dog_Heart/Valid'\n",
        "test_dataset_path = '/content/drive/My Drive/Dog_Heart/Test'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTpxpBlsqonX",
        "outputId": "7a8aba49-c5ca-4d62-bb8c-e265530fdc9c"
      },
      "id": "UTpxpBlsqonX",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "O1wLVPXRrjPn"
      },
      "id": "O1wLVPXRrjPn",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "572d9e51-7c67-43c6-be00-265ac66a9700",
      "metadata": {
        "id": "572d9e51-7c67-43c6-be00-265ac66a9700"
      },
      "source": [
        "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code)\n",
        "\n",
        "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
        "### (2) use custom dataloader for test dataset (return image tensor and file name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for Test Data\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('jpg', 'png', 'jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_path\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(train_dataset_path, transform=train_transforms)\n",
        "val_dataset = datasets.ImageFolder(val_dataset_path, transform=val_transforms)\n",
        "test_dataset = CustomTestDataset(root_dir=test_dataset_path, transform=test_transforms)\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(1024 * 1 * 1, 512)\n",
        "        self.fc2 = nn.Linear(512, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        x = self.pool(F.relu(self.conv6(x)))\n",
        "        x = self.pool(F.relu(self.conv7(x)))\n",
        "        x = x.view(-1, 1024 * 1 * 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 45\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train model\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Save the model if the validation loss decreased\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/UsualCnn_model.pth')\n",
        "        print(f'Saving model with validation loss {val_loss:.4f}')\n",
        "\n",
        "# Testing and Saving Results\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/UsualCnn_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "test_predictions = []\n",
        "image_paths = []\n",
        "with torch.no_grad():\n",
        "    for images, paths in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_predictions.extend(predicted.cpu().numpy())\n",
        "        image_paths.extend(paths)\n",
        "\n",
        "# Save predictions to CSV\n",
        "def save_predictions_to_csv(predictions, filenames, filename):\n",
        "    filenames = [os.path.basename(path) for path in filenames]\n",
        "    df = pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYfG0g8ksWy7",
        "outputId": "40dbecbd-012d-45f4-fea4-a778f21b6ca9"
      },
      "id": "AYfG0g8ksWy7",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/45], Train Loss: 1.0132, Val Loss: 1.0025\n",
            "Saving model with validation loss 1.0025\n",
            "Epoch [2/45], Train Loss: 1.0087, Val Loss: 0.9921\n",
            "Saving model with validation loss 0.9921\n",
            "Epoch [3/45], Train Loss: 0.9964, Val Loss: 0.9870\n",
            "Saving model with validation loss 0.9870\n",
            "Epoch [4/45], Train Loss: 0.9913, Val Loss: 0.9764\n",
            "Saving model with validation loss 0.9764\n",
            "Epoch [5/45], Train Loss: 0.9844, Val Loss: 0.9400\n",
            "Saving model with validation loss 0.9400\n",
            "Epoch [6/45], Train Loss: 0.9552, Val Loss: 0.9131\n",
            "Saving model with validation loss 0.9131\n",
            "Epoch [7/45], Train Loss: 0.9347, Val Loss: 0.8816\n",
            "Saving model with validation loss 0.8816\n",
            "Epoch [8/45], Train Loss: 0.9102, Val Loss: 0.8332\n",
            "Saving model with validation loss 0.8332\n",
            "Epoch [9/45], Train Loss: 0.9046, Val Loss: 0.7419\n",
            "Saving model with validation loss 0.7419\n",
            "Epoch [10/45], Train Loss: 0.8750, Val Loss: 0.7938\n",
            "Epoch [11/45], Train Loss: 0.8763, Val Loss: 0.7490\n",
            "Epoch [12/45], Train Loss: 0.8676, Val Loss: 0.7344\n",
            "Saving model with validation loss 0.7344\n",
            "Epoch [13/45], Train Loss: 0.8652, Val Loss: 0.7355\n",
            "Epoch [14/45], Train Loss: 0.8478, Val Loss: 0.7398\n",
            "Epoch [15/45], Train Loss: 0.8234, Val Loss: 0.7457\n",
            "Epoch [16/45], Train Loss: 0.8208, Val Loss: 0.7069\n",
            "Saving model with validation loss 0.7069\n",
            "Epoch [17/45], Train Loss: 0.8031, Val Loss: 0.6960\n",
            "Saving model with validation loss 0.6960\n",
            "Epoch [18/45], Train Loss: 0.8171, Val Loss: 0.6823\n",
            "Saving model with validation loss 0.6823\n",
            "Epoch [19/45], Train Loss: 0.8040, Val Loss: 0.6578\n",
            "Saving model with validation loss 0.6578\n",
            "Epoch [20/45], Train Loss: 0.7956, Val Loss: 0.6680\n",
            "Epoch [21/45], Train Loss: 0.7728, Val Loss: 0.7126\n",
            "Epoch [22/45], Train Loss: 0.8165, Val Loss: 0.6908\n",
            "Epoch [23/45], Train Loss: 0.7635, Val Loss: 0.6599\n",
            "Epoch [24/45], Train Loss: 0.7695, Val Loss: 0.6338\n",
            "Saving model with validation loss 0.6338\n",
            "Epoch [25/45], Train Loss: 0.7536, Val Loss: 0.6403\n",
            "Epoch [26/45], Train Loss: 0.7479, Val Loss: 0.8123\n",
            "Epoch [27/45], Train Loss: 0.7460, Val Loss: 0.6322\n",
            "Saving model with validation loss 0.6322\n",
            "Epoch [28/45], Train Loss: 0.7392, Val Loss: 0.6566\n",
            "Epoch [29/45], Train Loss: 0.7314, Val Loss: 0.6755\n",
            "Epoch [30/45], Train Loss: 0.7473, Val Loss: 0.7406\n",
            "Epoch [31/45], Train Loss: 0.7209, Val Loss: 0.6391\n",
            "Epoch [32/45], Train Loss: 0.7457, Val Loss: 0.6369\n",
            "Epoch [33/45], Train Loss: 0.7344, Val Loss: 0.7087\n",
            "Epoch [34/45], Train Loss: 0.7285, Val Loss: 0.7191\n",
            "Epoch [35/45], Train Loss: 0.7097, Val Loss: 0.6977\n",
            "Epoch [36/45], Train Loss: 0.6807, Val Loss: 0.6443\n",
            "Epoch [37/45], Train Loss: 0.7010, Val Loss: 0.6456\n",
            "Epoch [38/45], Train Loss: 0.7198, Val Loss: 0.6446\n",
            "Epoch [39/45], Train Loss: 0.6898, Val Loss: 0.6234\n",
            "Saving model with validation loss 0.6234\n",
            "Epoch [40/45], Train Loss: 0.6801, Val Loss: 0.6576\n",
            "Epoch [41/45], Train Loss: 0.6967, Val Loss: 0.7457\n",
            "Epoch [42/45], Train Loss: 0.7080, Val Loss: 0.6389\n",
            "Epoch [43/45], Train Loss: 0.6874, Val Loss: 0.6769\n",
            "Epoch [44/45], Train Loss: 0.6797, Val Loss: 0.6594\n",
            "Epoch [45/45], Train Loss: 0.6476, Val Loss: 0.6653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b668d4627b86>:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/My Drive/UsualCnn_model.pth'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to CSV without headers\n",
        "def save_predictions_to_csv(predictions, filenames, filename):\n",
        "    filenames = [os.path.basename(path) for path in filenames]\n",
        "    # Combine filenames and predictions into a list of tuples\n",
        "    data = list(zip(filenames, predictions))\n",
        "    # Write to CSV without headers\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False, header=False)\n",
        "\n",
        "# Save the results to Google Drive\n",
        "save_predictions_to_csv(test_predictions, image_paths, '/content/drive/My Drive/UsualCnn_test_predictions.csv')"
      ],
      "metadata": {
        "id": "CFPMB6oLshUH"
      },
      "id": "CFPMB6oLshUH",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bab9c27-159c-49c9-93bd-91152bd7152b",
      "metadata": {
        "id": "7bab9c27-159c-49c9-93bd-91152bd7152b"
      },
      "source": [
        "# 3. Evaluate your model using the developed software"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-11-19 185526.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "B3wQUC_92pLs"
      },
      "id": "B3wQUC_92pLs"
    },
    {
      "cell_type": "markdown",
      "id": "ae08c0c1-0af9-4f75-a9a9-783e8a94c87a",
      "metadata": {
        "id": "ae08c0c1-0af9-4f75-a9a9-783e8a94c87a"
      },
      "source": [
        "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 75%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24840ff0-8340-4040-aa06-bb312487a8e6",
      "metadata": {
        "id": "24840ff0-8340-4040-aa06-bb312487a8e6"
      },
      "source": [
        "### Comparison of UsualCNN and RVT\n",
        "\n",
        "#### **Performance Overview**\n",
        "- **UsualCNN**: Achieves 70.5% accuracy, outperforming AlexNet (68.3%) and VGG16 (69.5%) but below the 75% benchmark.\n",
        "- **RVT**: Surpasses 75% due to attention mechanisms, residual connections, and pretraining on large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why RVT Performs Better**\n",
        "1. **Attention Mechanisms**: Focuses on key image regions.\n",
        "2. **Residual Connections**: Enables efficient training of deep networks.\n",
        "3. **Pretraining**: Leverages large datasets for strong feature extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Improve UsualCNN**\n",
        "1. Add **attention mechanisms** (CBAM, SE-Blocks).\n",
        "2. Use **transfer learning** with pre-trained models.\n",
        "3. Expand the dataset or use advanced **data augmentation** (MixUp, CutMix).\n",
        "4. Add **residual connections** for better gradient flow.\n",
        "5. Optimize **hyperparameters** and regularization techniques.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "With these enhancements, UsualCNN can achieve or exceed 75%, making it competitive with RVT and suitable for more advanced tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc93859c-b7a9-4313-9ca6-730a3e07c555",
      "metadata": {
        "id": "cc93859c-b7a9-4313-9ca6-730a3e07c555"
      },
      "source": [
        "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResearchGate link: https://www.researchgate.net/publication/385944507_Implementation_and_Analysis_of_a_Custom_CNN_Architecture_for_Dog_Heart_Image_Classification"
      ],
      "metadata": {
        "id": "hiicZ4JJ2Sqr"
      },
      "id": "hiicZ4JJ2Sqr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github link: https://github.com/priyadarshini-777/Implementation-and-Analysis-of-a-Custom-CNN-Architecture-for-Dog-Heart-Image-Classification"
      ],
      "metadata": {
        "id": "cUCvDMtx2TB9"
      },
      "id": "cUCvDMtx2TB9"
    },
    {
      "cell_type": "markdown",
      "id": "a0f67144-4066-4541-9d7f-5bc6432bdb6b",
      "metadata": {
        "id": "a0f67144-4066-4541-9d7f-5bc6432bdb6b"
      },
      "source": [
        "# 6. Grading rubric\n",
        "\n",
        "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
        "\n",
        "(2). Grammer ---- 20 points\n",
        "\n",
        "(3). Introduction & related work --- 10 points\n",
        "\n",
        "\n",
        "(4). Method  ---- 20 points\n",
        "\n",
        "(5). Results ---- 20 points\n",
        "\n",
        "     > = 70 % -->10 points\n",
        "     < 50 % -->0 points\n",
        "     >= 50 % & < 70% --> 0.5 point/percent\n",
        "     \n",
        "\n",
        "(6). Discussion - 10 points"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}